{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare filelists for LJSpeech dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See: https://github.com/espeak-ng/espeak-ng/blob/master/docs/languages.md\n",
    "dir_data = \"../../../dataset_uz\"\n",
    "audio_dir = \"dataset_uz/audio\"\n",
    "config = \"../config.yaml\"\n",
    "# symlink = \"DUMMY3\"\n",
    "n_val = 100\n",
    "n_test = 500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get hyperparameters from config file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from utils.hparams import get_hparams_from_file\n",
    "\n",
    "hps = get_hparams_from_file(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check espeak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello world -> hællɔ (en)dʌbəljuː(uz) o eɹ el de \n"
     ]
    }
   ],
   "source": [
    "from phonemizer import phonemize\n",
    "\n",
    "print('hello world ->', phonemize('hello world', backend='espeak', language='uz'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read dataset\n",
    "\n",
    "Here `normalized_text` contains numbers in the form of words.\n",
    "\n",
    "**Note**: you may need to replace all `\"|\"` with `\" | \"` in the file `metadata.csv`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>text</th>\n",
       "      <th>normalized_text</th>\n",
       "      <th>cleaned_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>dataset_uz/audio/utt_0000.wav</td>\n",
       "      <td>U do‘kondan non sotib oldi.</td>\n",
       "      <td>u do‘kondan non sotib oldi.</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dataset_uz/audio/utt_0001.wav</td>\n",
       "      <td>Bugun havo juda iliq.</td>\n",
       "      <td>bugun havo juda iliq.</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dataset_uz/audio/utt_0002.wav</td>\n",
       "      <td>kVt.s elektr energiya yetkazib bergan.</td>\n",
       "      <td>kilovatt-soniya elektr energiya yetkazib bergan.</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dataset_uz/audio/utt_0003.wav</td>\n",
       "      <td>Joriy yilda bu raqam 2 mlrd kVt.soatni tashkil...</td>\n",
       "      <td>joriy yilda bu raqam ikki milliard kilovatt.so...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>dataset_uz/audio/utt_0004.wav</td>\n",
       "      <td>Yangi elektr uzatish liniyasini barpo etish, O...</td>\n",
       "      <td>yangi elektr uzatish liniyasini barpo etish, o...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>dataset_uz/audio/utt_0005.wav</td>\n",
       "      <td>Mustaqil Davlatlar Hamdo‘stligi (MDH) missiyas...</td>\n",
       "      <td>mustaqil davlatlar hamdo‘stligi (mdh) missiyas...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>dataset_uz/audio/utt_0006.wav</td>\n",
       "      <td>Bu haqda MDH Ijroiya qo‘mitasi raisi Sergey Le...</td>\n",
       "      <td>bu haqda mdh ijroiya qo‘mitasi raisi sergey le...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>dataset_uz/audio/utt_0007.wav</td>\n",
       "      <td>\"17 noyabr kuni MDH missiyasi O‘zbekistonda is...</td>\n",
       "      <td>oʻn yettinchi noyabr kuni mdh missiyasi o‘zbek...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>dataset_uz/audio/utt_0008.wav</td>\n",
       "      <td>O‘n nafardan ortiq uzoq muddatli va yana 700 n...</td>\n",
       "      <td>o‘n nafardan ortiq uzoq muddatli va yana yetti...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>dataset_uz/audio/utt_0009.wav</td>\n",
       "      <td>Ularning deyarli barchasi MDH mamlakatlari vak...</td>\n",
       "      <td>ularning deyarli barchasi mdh mamlakatlari vak...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            file  \\\n",
       "0  dataset_uz/audio/utt_0000.wav   \n",
       "1  dataset_uz/audio/utt_0001.wav   \n",
       "2  dataset_uz/audio/utt_0002.wav   \n",
       "3  dataset_uz/audio/utt_0003.wav   \n",
       "4  dataset_uz/audio/utt_0004.wav   \n",
       "5  dataset_uz/audio/utt_0005.wav   \n",
       "6  dataset_uz/audio/utt_0006.wav   \n",
       "7  dataset_uz/audio/utt_0007.wav   \n",
       "8  dataset_uz/audio/utt_0008.wav   \n",
       "9  dataset_uz/audio/utt_0009.wav   \n",
       "\n",
       "                                                text  \\\n",
       "0                        U do‘kondan non sotib oldi.   \n",
       "1                              Bugun havo juda iliq.   \n",
       "2             kVt.s elektr energiya yetkazib bergan.   \n",
       "3  Joriy yilda bu raqam 2 mlrd kVt.soatni tashkil...   \n",
       "4  Yangi elektr uzatish liniyasini barpo etish, O...   \n",
       "5  Mustaqil Davlatlar Hamdo‘stligi (MDH) missiyas...   \n",
       "6  Bu haqda MDH Ijroiya qo‘mitasi raisi Sergey Le...   \n",
       "7  \"17 noyabr kuni MDH missiyasi O‘zbekistonda is...   \n",
       "8  O‘n nafardan ortiq uzoq muddatli va yana 700 n...   \n",
       "9  Ularning deyarli barchasi MDH mamlakatlari vak...   \n",
       "\n",
       "                                     normalized_text  cleaned_text  \n",
       "0                        u do‘kondan non sotib oldi.           NaN  \n",
       "1                              bugun havo juda iliq.           NaN  \n",
       "2   kilovatt-soniya elektr energiya yetkazib bergan.           NaN  \n",
       "3  joriy yilda bu raqam ikki milliard kilovatt.so...           NaN  \n",
       "4  yangi elektr uzatish liniyasini barpo etish, o...           NaN  \n",
       "5  mustaqil davlatlar hamdo‘stligi (mdh) missiyas...           NaN  \n",
       "6  bu haqda mdh ijroiya qo‘mitasi raisi sergey le...           NaN  \n",
       "7  oʻn yettinchi noyabr kuni mdh missiyasi o‘zbek...           NaN  \n",
       "8  o‘n nafardan ortiq uzoq muddatli va yana yetti...           NaN  \n",
       "9  ularning deyarli barchasi mdh mamlakatlari vak...           NaN  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data = pd.read_csv(\n",
    "#     f\"{dir_data}/uzbek_tts_output.csv\",\n",
    "#     sep=r\"|\",\n",
    "#     header=None,\n",
    "#     names=[\"file\", \"text\", \"normalized_text\", \"cleaned_text\"],\n",
    "#     index_col=False,\n",
    "#     # converter to add .wav to file name\n",
    "#     converters={\"file\": lambda x: f\"{symlink}/{x.strip()}.wav\", \"text\": str.strip, \"normalized_text\": str.strip},\n",
    "# )\n",
    "# data.head(10)\n",
    "\n",
    "#symlinnkni o'chirish kerak\n",
    "data = pd.read_csv(\n",
    "    f\"{dir_data}/uzbek_tts_output.csv\",\n",
    "    sep=r\"|\",\n",
    "    header=None,\n",
    "    names=[\"file\", \"text\", \"normalized_text\", \"cleaned_text\"],\n",
    "    index_col=False,\n",
    "    # converter to add .wav to file name\n",
    "    converters={\"file\": lambda x: f\"{audio_dir}/{x.strip()}.wav\", \"text\": str.strip, \"normalized_text\": str.strip},\n",
    ")\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text cleaners\n",
    "\n",
    "It may take a while, so better to preprocess the text and save it to a file in advance.\n",
    "\n",
    "**Note** `phonemize_text` takes the longest time.`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tokenize_text', 'add_bos_eos']\n",
      "[['phonemize_text'], ['add_spaces']]\n"
     ]
    }
   ],
   "source": [
    "# Get index of tokenize_text\n",
    "text_cleaners = hps.data.text_cleaners\n",
    "\n",
    "token_idx = text_cleaners.index(\"tokenize_text\")\n",
    "token_cleaners = text_cleaners[token_idx:]\n",
    "print(token_cleaners)\n",
    "\n",
    "\n",
    "# Extract phonemize_text\n",
    "def separate_text_cleaners(text_cleaners):\n",
    "    final_list = []\n",
    "    temp_list = []\n",
    "\n",
    "    for cleaner in text_cleaners:\n",
    "        if cleaner == \"phonemize_text\":\n",
    "            if temp_list:\n",
    "                final_list.append(temp_list)\n",
    "            final_list.append([cleaner])\n",
    "            temp_list = []\n",
    "        else:\n",
    "            temp_list.append(cleaner)\n",
    "\n",
    "    if temp_list:\n",
    "        final_list.append(temp_list)\n",
    "\n",
    "    return final_list\n",
    "\n",
    "\n",
    "text_cleaners = text_cleaners[:token_idx]\n",
    "text_cleaners = separate_text_cleaners(text_cleaners)\n",
    "print(text_cleaners)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning with ['phonemize_text'] ...\n",
      "Cleaning with ['add_spaces'] ...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>text</th>\n",
       "      <th>normalized_text</th>\n",
       "      <th>cleaned_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>dataset_uz/audio/utt_0000.wav</td>\n",
       "      <td>U do‘kondan non sotib oldi.</td>\n",
       "      <td>u do‘kondan non sotib oldi.</td>\n",
       "      <td>ʊ &lt;space&gt; d ɔ ʔ k ˈɔ n d a n &lt;space&gt; n ˈɔ n &lt;s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dataset_uz/audio/utt_0001.wav</td>\n",
       "      <td>Bugun havo juda iliq.</td>\n",
       "      <td>bugun havo juda iliq.</td>\n",
       "      <td>b ˈu ɡ ʊ n &lt;space&gt; h ˈa v ɔ &lt;space&gt; j ˈu d a &lt;...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dataset_uz/audio/utt_0002.wav</td>\n",
       "      <td>kVt.s elektr energiya yetkazib bergan.</td>\n",
       "      <td>kilovatt-soniya elektr energiya yetkazib bergan.</td>\n",
       "      <td>k i l ˈo v a t t s o n ˈi j a &lt;space&gt; ˈe l ɛ k...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dataset_uz/audio/utt_0003.wav</td>\n",
       "      <td>Joriy yilda bu raqam 2 mlrd kVt.soatni tashkil...</td>\n",
       "      <td>joriy yilda bu raqam ikki milliard kilovatt.so...</td>\n",
       "      <td>j ˈo ɹ ɪ j &lt;space&gt; j ˈɪ l d a &lt;space&gt; b ʊ &lt;spa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>dataset_uz/audio/utt_0004.wav</td>\n",
       "      <td>Yangi elektr uzatish liniyasini barpo etish, O...</td>\n",
       "      <td>yangi elektr uzatish liniyasini barpo etish, o...</td>\n",
       "      <td>j ˈa ŋ ɪ &lt;space&gt; ˈe l ɛ k t r &lt;space&gt; u z ˈa t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            file  \\\n",
       "0  dataset_uz/audio/utt_0000.wav   \n",
       "1  dataset_uz/audio/utt_0001.wav   \n",
       "2  dataset_uz/audio/utt_0002.wav   \n",
       "3  dataset_uz/audio/utt_0003.wav   \n",
       "4  dataset_uz/audio/utt_0004.wav   \n",
       "\n",
       "                                                text  \\\n",
       "0                        U do‘kondan non sotib oldi.   \n",
       "1                              Bugun havo juda iliq.   \n",
       "2             kVt.s elektr energiya yetkazib bergan.   \n",
       "3  Joriy yilda bu raqam 2 mlrd kVt.soatni tashkil...   \n",
       "4  Yangi elektr uzatish liniyasini barpo etish, O...   \n",
       "\n",
       "                                     normalized_text  \\\n",
       "0                        u do‘kondan non sotib oldi.   \n",
       "1                              bugun havo juda iliq.   \n",
       "2   kilovatt-soniya elektr energiya yetkazib bergan.   \n",
       "3  joriy yilda bu raqam ikki milliard kilovatt.so...   \n",
       "4  yangi elektr uzatish liniyasini barpo etish, o...   \n",
       "\n",
       "                                        cleaned_text  \n",
       "0  ʊ <space> d ɔ ʔ k ˈɔ n d a n <space> n ˈɔ n <s...  \n",
       "1  b ˈu ɡ ʊ n <space> h ˈa v ɔ <space> j ˈu d a <...  \n",
       "2  k i l ˈo v a t t s o n ˈi j a <space> ˈe l ɛ k...  \n",
       "3  j ˈo ɹ ɪ j <space> j ˈɪ l d a <space> b ʊ <spa...  \n",
       "4  j ˈa ŋ ɪ <space> ˈe l ɛ k t r <space> u z ˈa t...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from text import tokenizer\n",
    "from torchtext.vocab import Vocab\n",
    "\n",
    "text_norm = data[\"normalized_text\"].tolist()\n",
    "for cleaners in text_cleaners:\n",
    "    print(f\"Cleaning with {cleaners} ...\")\n",
    "    if cleaners[0] == \"phonemize_text\":\n",
    "        text_norm = tokenizer(text_norm, Vocab, cleaners, language=hps.data.language)\n",
    "    else:\n",
    "        for idx, text in enumerate(text_norm):\n",
    "            temp = tokenizer(text, Vocab, cleaners, language=hps.data.language)\n",
    "            text_norm[idx] = temp\n",
    "\n",
    "data = data.assign(cleaned_text=text_norm)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate and save vocabulary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of vocabulary: 90\n",
      "['<pad>', '<unk>', '<bos>', '<eos>', '<space>', '<laugh>', 'a', 'ɪ', 'l', 'n', 's', 't', 'ˈa', 'd', 'm', 'j', 'k', 'b', 'h', 'r', 'ɹ', 'ˈi', 'ɡ', 'q', 'ˈɪ', 'ɔ', 'ˈɔ', 'z', 'ʔ', 'i', 'ˌa', 'v', '.', 'tʃ', 'o', 'ʊ', 'ŋ', 'ˈo', 'ˈʊ', ',', 'ˌɔ', 'ˌi', 'χ', 'p', 'f', 'ˌu', 'u', 'ˌo', 'e', 'ˈe', 'ˈu', 'ˈɛ', 'ˌɪ', 'ɛ', 'ˌe', 'ˈæ', 'æ', 'ˌʊ', 'ˌɛ', '“', '”', 'ˌæ', 'ts', ':', '(en)', '(uz)', '?', '!', 'ə', 'ˈʌ', 'əl', 'ˌuː', 'ʃ', ';', 'ɐ', '\"', 'ˈy', 'x', 'ɯ', 'ˈaɪ', 'ˈɔː', '—', '…', 'oː', 'ɔː', 'c', 'ɟ', 'ˈeɪ', 'ˌɒ', 'ˌɔː']\n"
     ]
    }
   ],
   "source": [
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from utils.task import load_vocab, save_vocab\n",
    "from text.symbols import special_symbols, UNK_ID\n",
    "from typing import List\n",
    "\n",
    "\n",
    "def yield_tokens(cleaned_text: List[str]):\n",
    "    for text in cleaned_text:\n",
    "        yield text.split()\n",
    "\n",
    "\n",
    "text_norm = data[\"cleaned_text\"].tolist()\n",
    "vocab = build_vocab_from_iterator(yield_tokens(text_norm), specials=special_symbols)\n",
    "vocab.set_default_index(UNK_ID)\n",
    "\n",
    "vocab_file = f\"../vocab_uz.txt\"\n",
    "save_vocab(vocab, vocab_file)\n",
    "\n",
    "vocab = load_vocab(vocab_file)\n",
    "print(f\"Size of vocabulary: {len(vocab)}\")\n",
    "print(vocab.get_itos())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Token cleaners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>text</th>\n",
       "      <th>normalized_text</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>dataset_uz/audio/utt_0000.wav</td>\n",
       "      <td>U do‘kondan non sotib oldi.</td>\n",
       "      <td>u do‘kondan non sotib oldi.</td>\n",
       "      <td>ʊ &lt;space&gt; d ɔ ʔ k ˈɔ n d a n &lt;space&gt; n ˈɔ n &lt;s...</td>\n",
       "      <td>2\\t35\\t4\\t13\\t25\\t28\\t16\\t26\\t9\\t13\\t6\\t9\\t4\\t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dataset_uz/audio/utt_0001.wav</td>\n",
       "      <td>Bugun havo juda iliq.</td>\n",
       "      <td>bugun havo juda iliq.</td>\n",
       "      <td>b ˈu ɡ ʊ n &lt;space&gt; h ˈa v ɔ &lt;space&gt; j ˈu d a &lt;...</td>\n",
       "      <td>2\\t17\\t50\\t22\\t35\\t9\\t4\\t18\\t12\\t31\\t25\\t4\\t15...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dataset_uz/audio/utt_0002.wav</td>\n",
       "      <td>kVt.s elektr energiya yetkazib bergan.</td>\n",
       "      <td>kilovatt-soniya elektr energiya yetkazib bergan.</td>\n",
       "      <td>k i l ˈo v a t t s o n ˈi j a &lt;space&gt; ˈe l ɛ k...</td>\n",
       "      <td>2\\t16\\t29\\t8\\t37\\t31\\t6\\t11\\t11\\t10\\t34\\t9\\t21...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dataset_uz/audio/utt_0003.wav</td>\n",
       "      <td>Joriy yilda bu raqam 2 mlrd kVt.soatni tashkil...</td>\n",
       "      <td>joriy yilda bu raqam ikki milliard kilovatt.so...</td>\n",
       "      <td>j ˈo ɹ ɪ j &lt;space&gt; j ˈɪ l d a &lt;space&gt; b ʊ &lt;spa...</td>\n",
       "      <td>2\\t15\\t37\\t20\\t7\\t15\\t4\\t15\\t24\\t8\\t13\\t6\\t4\\t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>dataset_uz/audio/utt_0004.wav</td>\n",
       "      <td>Yangi elektr uzatish liniyasini barpo etish, O...</td>\n",
       "      <td>yangi elektr uzatish liniyasini barpo etish, o...</td>\n",
       "      <td>j ˈa ŋ ɪ &lt;space&gt; ˈe l ɛ k t r &lt;space&gt; u z ˈa t...</td>\n",
       "      <td>2\\t15\\t12\\t36\\t7\\t4\\t49\\t8\\t53\\t16\\t11\\t19\\t4\\...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            file  \\\n",
       "0  dataset_uz/audio/utt_0000.wav   \n",
       "1  dataset_uz/audio/utt_0001.wav   \n",
       "2  dataset_uz/audio/utt_0002.wav   \n",
       "3  dataset_uz/audio/utt_0003.wav   \n",
       "4  dataset_uz/audio/utt_0004.wav   \n",
       "\n",
       "                                                text  \\\n",
       "0                        U do‘kondan non sotib oldi.   \n",
       "1                              Bugun havo juda iliq.   \n",
       "2             kVt.s elektr energiya yetkazib bergan.   \n",
       "3  Joriy yilda bu raqam 2 mlrd kVt.soatni tashkil...   \n",
       "4  Yangi elektr uzatish liniyasini barpo etish, O...   \n",
       "\n",
       "                                     normalized_text  \\\n",
       "0                        u do‘kondan non sotib oldi.   \n",
       "1                              bugun havo juda iliq.   \n",
       "2   kilovatt-soniya elektr energiya yetkazib bergan.   \n",
       "3  joriy yilda bu raqam ikki milliard kilovatt.so...   \n",
       "4  yangi elektr uzatish liniyasini barpo etish, o...   \n",
       "\n",
       "                                        cleaned_text  \\\n",
       "0  ʊ <space> d ɔ ʔ k ˈɔ n d a n <space> n ˈɔ n <s...   \n",
       "1  b ˈu ɡ ʊ n <space> h ˈa v ɔ <space> j ˈu d a <...   \n",
       "2  k i l ˈo v a t t s o n ˈi j a <space> ˈe l ɛ k...   \n",
       "3  j ˈo ɹ ɪ j <space> j ˈɪ l d a <space> b ʊ <spa...   \n",
       "4  j ˈa ŋ ɪ <space> ˈe l ɛ k t r <space> u z ˈa t...   \n",
       "\n",
       "                                              tokens  \n",
       "0  2\\t35\\t4\\t13\\t25\\t28\\t16\\t26\\t9\\t13\\t6\\t9\\t4\\t...  \n",
       "1  2\\t17\\t50\\t22\\t35\\t9\\t4\\t18\\t12\\t31\\t25\\t4\\t15...  \n",
       "2  2\\t16\\t29\\t8\\t37\\t31\\t6\\t11\\t11\\t10\\t34\\t9\\t21...  \n",
       "3  2\\t15\\t37\\t20\\t7\\t15\\t4\\t15\\t24\\t8\\t13\\t6\\t4\\t...  \n",
       "4  2\\t15\\t12\\t36\\t7\\t4\\t49\\t8\\t53\\t16\\t11\\t19\\t4\\...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from text import detokenizer\n",
    "\n",
    "text_norm = data[\"cleaned_text\"].tolist()\n",
    "for idx, text in enumerate(text_norm):\n",
    "    temp = tokenizer(text, vocab, token_cleaners, language=hps.data.language)\n",
    "    assert UNK_ID not in temp, f\"Found unknown symbol:\\n{text}\\n{detokenizer(temp)}\"\n",
    "    text_norm[idx] = temp\n",
    "\n",
    "text_norm = [\"\\t\".join(map(str, text)) for text in text_norm]\n",
    "data = data.assign(tokens=text_norm)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save train, val, test filelists\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[[\"file\", \"tokens\"]]\n",
    "data = data.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "data_train = data.iloc[n_val + n_test:]\n",
    "data_val = data.iloc[:n_val]\n",
    "data_test = data.iloc[n_val: n_val + n_test]\n",
    "\n",
    "data_train.to_csv(\"../filelists/train.txt\", sep=\"|\", index=False, header=False)\n",
    "data_val.to_csv(\"../filelists/val.txt\", sep=\"|\", index=False, header=False)\n",
    "data_test.to_csv(\"../filelists/test.txt\", sep=\"|\", index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
